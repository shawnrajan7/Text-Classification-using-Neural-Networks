{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant libraries\n",
    "import numpy as np\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Embedding, Input, Dense, Bidirectional, LSTM, Layer, GRU, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import keras.callbacks\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to load the data\n",
    "def load_data(path_to_dir, dataset):\n",
    "    \n",
    "    '''\n",
    "    The loading procedure is different for the two datasets, therefore we add a parameter to the load \n",
    "    function that specifies the dataset to load\n",
    "    '''\n",
    "    \n",
    "    # Regular Expression to remove punctuation\n",
    "    Remove_punctuation = re.compile('[%s]' % re.escape(string.punctuation.replace('.',''))) #We leave the periods in since we want to split it into sentences\n",
    "    \n",
    "    if dataset == \"imdb\":\n",
    "        \n",
    "        train_pos = []\n",
    "        train_neg = []\n",
    "        test_pos = []\n",
    "        test_neg = []\n",
    "\n",
    "        for filename in os.listdir(path_to_dir+\"/train/pos/\"):\n",
    "            train_pos.append(open(path_to_dir+\"/train/pos/\"+filename,'r',encoding='utf-8').read())\n",
    "\n",
    "        for filename in os.listdir(path_to_dir+\"/train/neg/\"):\n",
    "            train_neg.append(open(path_to_dir+\"/train/neg/\"+filename,'r',encoding='utf-8').read())\n",
    "\n",
    "        for filename in os.listdir(path_to_dir+\"/test/pos/\"):\n",
    "            test_pos.append(open(path_to_dir+\"/test/pos/\"+filename,'r',encoding='utf-8').read())\n",
    "\n",
    "        for filename in os.listdir(path_to_dir+\"/test/neg/\"):\n",
    "            test_neg.append(open(path_to_dir+\"/test/neg/\"+filename,'r',encoding='utf-8').read())\n",
    "        \n",
    "        train_pos = [Remove_punctuation.sub('',i).lower() for i in train_pos]\n",
    "        train_neg = [Remove_punctuation.sub('',i).lower() for i in train_neg]\n",
    "        test_pos = [Remove_punctuation.sub('',i).lower() for i in test_pos]\n",
    "        test_neg = [Remove_punctuation.sub('',i).lower() for i in test_neg]\n",
    "        \n",
    "        #train_pos = [[sent for sent in review if len(sent)!=0] for review in train_pos]\n",
    "        #train_neg = [[sent for sent in review if len(sent)!=0] for review in train_pos]\n",
    "        #test_pos = [[sent for sent in review if len(sent)!=0] for review in train_pos]\n",
    "        #test_neg = [[sent for sent in review if len(sent)!=0] for review in train_pos]\n",
    "        \n",
    "        X_train = train_pos + train_neg\n",
    "        X_test = test_pos + test_neg\n",
    "        \n",
    "        Y_train = [1]*len(train_pos) + [0]*len(train_neg)\n",
    "        Y_test = [1]*len(test_pos) + [0]*len(test_neg)        \n",
    "        \n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "        \n",
    "    if dataset == \"yelp\":\n",
    "        \n",
    "        Train_data = pd.read_csv(path_to_dir + \"Yelp_train_data.csv\")\n",
    "        Train_data = Train_data[Train_data['stars']!=3]\n",
    "        \n",
    "        X_train = Train_data['text']\n",
    "        X_train = list(X_train)\n",
    "        \n",
    "        Y_train = Train_data['stars']\n",
    "        Y_train = list(Y_train)\n",
    "        Y_train = [0 if i <= 3 else 1 for i in Y_train]\n",
    "        \n",
    "        Test_data =  pd.read_csv(path_to_dir + \"Yelp_test_data.csv\")\n",
    "        Test_data = Test_data[Test_data['stars']!=3]\n",
    "        \n",
    "        X_test = Test_data['text']\n",
    "        X_test = list(X_test)\n",
    "        \n",
    "        Y_test = Test_data['stars']\n",
    "        Y_test = list(Y_test)\n",
    "        Y_test = [0 if i <=3 else 1 for i in Y_test]\n",
    "        \n",
    "        X_train = [Remove_punctuation.sub('',i).lower() for i in X_train]\n",
    "        X_test = [Remove_punctuation.sub('',i).lower() for i in X_test]\n",
    "        \n",
    "        #X_train = [[sent for sent in review if len(sent)!=0] for review in X_train]\n",
    "        #X_test = [[sent for sent in review if len(sent)!=0] for review in X_test]\n",
    "               \n",
    "        return X_train, Y_train, X_test, Y_test       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data('../../../Data/IMDB Stanford/', \"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the reviews into sentences\n",
    "X_train_sentences = [review.split('.') for review in X_train]\n",
    "X_train_sentences = [[sentence for sentence in review if len(sentence)!=0] for review in X_train_sentences]\n",
    "\n",
    "X_test_sentences = [review.split('.') for review in X_test]\n",
    "X_test_sentences = [[sentence for sentence in review if len(sentence)!=0] for review in X_test_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters for the word embeddings\n",
    "\n",
    "Max_Words = 20000 #Max words in the vocabulary\n",
    "Max_Sequence_length = 100 #Max number of words in a sentence\n",
    "Embedding_dimension = 100 #Using 100 dimensional Glove data\n",
    "Max_Sentence_length = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 107359 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Converting the words into tokens\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = np.zeros((len(X_train), Max_Sentence_length, Max_Sequence_length), dtype='int32')\n",
    "#(Number of reviews, number of sentences per review = 15, number of words per sentence = 100)\n",
    "\n",
    "for i, sentences in enumerate(X_train_sentences):#Taking a single review(list of sentences) from a list of reviews\n",
    "    for j, sent in enumerate(sentences):#Iterating through sentences within a review. sent = a sentence in a single review\n",
    "        if j < Max_Sentence_length:\n",
    "            wordTokens = text_to_word_sequence(sent)#Converting sentences into words(similar function to .split())\n",
    "            #wordTokens is a list\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):#Iterate through each word in the word token\n",
    "                if k < Max_Sequence_length and tokenizer.word_index[word] < Max_Words:\n",
    "                    Train_data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "                    \n",
    "Test_data = np.zeros((len(X_test), Max_Sentence_length, Max_Sequence_length), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(X_test_sentences):#Taking a single review(list of sentences) from a list of reviews\n",
    "    for j, sent in enumerate(sentences):#Iterating through sentences within a review. sent = a sentence in a single review\n",
    "        if j < Max_Sentence_length:\n",
    "            wordTokens = text_to_word_sequence(sent)#Converting sentences into words(similar function to .split())\n",
    "            #wordTokens is a list\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):#Iterate through each word in the word token\n",
    "                if k < Max_Sequence_length and tokenizer.word_index.get(word,0) < Max_Words:\n",
    "                    Test_data[i, j, k] = tokenizer.word_index.get(word,0)\n",
    "                    k = k + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the labels\n",
    "Train_labels = np.array(Y_train)\n",
    "Test_labels = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        #self.built = True\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 483s 19ms/step\n",
      "acc: 87.20%\n"
     ]
    }
   ],
   "source": [
    "# Loading the trained model and measuring accuracy on test data\n",
    "HAN_model = load_model('HAN_model.h5', custom_objects = {'Attention': Attention})\n",
    "\n",
    "# Evaluating accuracy on Test data\n",
    "Test_score = HAN_model.evaluate(Test_data, Test_labels, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (HAN_model.metrics_names[1], Test_score[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
